# -*- coding: utf-8 -*-
"""etl.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1M1MbphRkaihLYyXzVMccq6u-o9mlNnUb
"""

import configparser
import os

from pyspark.sql import SparkSession

import clean_functions
import create_functions

config = configparser.ConfigParser()
config.read('config.cfg')

os.environ['AWS_ACCESS_KEY_ID'] = config['AWS']['AWS_ACCESS_KEY_ID']
os.environ['AWS_SECRET_ACCESS_KEY'] = config['AWS']['AWS_SECRET_ACCESS_KEY']


def create_spark_session():
    spark = SparkSession \
        .builder \
        .config("spark.jars.packages", "saurfang:spark-sas7bdat:2.0.0-s_2.11") \
        .enableHiveSupport() \
        .getOrCreate()
        
    return spark

def process_land_temperatures(spark, input_data, file_name):
    '''
      Process the global land temperatures data and return a dataframe
    '''

    # instantiate temperature data
    temperature_file_path = input_data + file_name

    # reads temperature data file into dataframe
    temp_df = spark.read.csv(temperature_file_path, header=True, inferSchema=True)

    # clean temperature dataframe
    temperature_df = clean_functions.clean_temperature_spark_df(temp_df)

    return temperature_df


def process_immigration_data(spark, input_data, output_data, file_name, temperature_file, mapping_file):
    '''
      Process immigration data
      Creates fact, arrivals, visa and country tables
    '''
    
    # instantiate immigration data
    immigration_file_path = input_data + file_name

    # reads immigration data file
    immigration_df = spark.read.format('com.github.saurfang.sas.spark').load(immigration_file_path)

    # clean immigration dataframe
    immigration_df = clean_functions.clean_immigration_spark_df(immigration_df)

    # create visa_type  table
    visa_type_df = create_functions.create_visa_dimension_table(immigration_df, output_data)

    # create arrivals  table
    arrivals_df = create_functions.create_immigration_arrivals_dimension(immigration_df, output_data)

    # get global temperatures data
    temperatures_df = process_land_temperatures(spark, input_data, temperature_file)

    # create country table
    country_df = create_functions.create_country_dimension_table(spark, immigration_df, temperatures_df, output_data, mapping_file)

    # create immigration fact table
    immi_fact_df = create_functions.create_immigration_fact_table(spark, immigration_df, output_data)


def process_demographics_data(spark, input_data, output_data, file_name):
    '''
      Process demographics data and create demographics table
    '''

    # instantiate demographics data
    demographics_file_path = input_data + file_name

    # reads demographics data file into dataframe
    temp_dem_df = spark.read.csv(demographics_file_path, inferSchema=True, header=True, sep=';')

    # clean demographics data
    cleaned_demographics_df = clean_functions.clean_demographics_spark_df(temp_dem_df)

    # create demographic table
    demographics_df = create_functions.create_demographics_dimension_table(cleaned_demographics_df, output_data)



def main():
    '''
      Calls on above functions
      Defines S3 bucket links
    '''
    
    spark = create_spark_session()

    input_data = "s3://udacity-capstone-input/"
    output_data = "s3://udacity-capstone-output/"

    immigration_file = 'i94_apr16_sub.sas7bdat'
    temperature_file = 'GlobalLandTemperaturesByCity.csv'
    usa_demographics_file = 'us-cities-demographics.csv'

    country_codes_file = input_data + "i94res.csv"

    country_codes_df = spark.read.csv(country_codes_file, header=True, inferSchema=True)

    process_immigration_data(spark, input_data, output_data, immigration_file, temperature_file, country_codes_df)

    process_demographics_data(spark, input_data, output_data, usa_demographics_file)


if __name__ == "__main__":
    main()